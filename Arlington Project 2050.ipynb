{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello there, the following code is made to clean up and visual the data set from the Hispanic Heritage Community Festival. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For context, data was collected asking Arlington residents questions related to the Arlington and how or where they see Arlington in the year 2050. The data gathered was in english, spanish, and voice messsges. In order to sort through the data I needed to first clean up the excel file in order to create graphs ands chart highlighting the results found. For this I used python and pandas to clean up the excel file, and seaborn, spacytextblob, and matplotlib to visualize the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m pip install --upgrade matplotlib\n",
    "python3 -m pip install --upgrade wordcloud\n",
    "python3 -m pip install --upgrade numpy\n",
    "python3 -m pip install --upgrade spacy\n",
    "python3 -m pip install --upgrade spacytextblob\n",
    "python3 -m pip install --upgrade textblob\n",
    "python3 -m pip install --upgrade pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob  \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('spacytextblob')\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hispanic = pd.read_excel(\"Hispanic Heritage data.xlsx\")\n",
    "pd.read_excel(\"Hispanic Heritage data.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 4 boxes of python here are meant to clean up the headings for easier access and legibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hispanic.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hispanic = df_hispanic.drop([0, 1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hispanic.columns = ['id', 'first', 'first_translated', 'second', 'second_translated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code looks through 4 coloumns that are all intedned to store data from 1 row/response. Given some text needed to be translated and some did not, there were many coloumns left blank. This is why in the next block of code a new coloumn with all the data is made for easy access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_text(row):\n",
    "    text = \"\"\n",
    "    if pd.notna(row['first_translated']):\n",
    "        text += row['first_translated']\n",
    "    elif pd.notna(row['first']):\n",
    "        text += row['first']\n",
    "    \n",
    "    if pd.notna(row['second_translated']):\n",
    "        text += \". \" + row['second_translated']\n",
    "    elif pd.notna(row['second']):\n",
    "        text += \". \" + row['second'] \n",
    "\n",
    "    return text # This returns the text of the given row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses Sentiment Analyses to determine the polarity of the code, meaning how positive or negative is the given word or words. It also creates a new coloumn to store this text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hispanic['concatenated_text'] = df_hispanic.apply(concatenate_text, axis=1) # new coloumn of all the data\n",
    "df_hispanic['source'] = \"Hispanic Heritage Fest\"\n",
    "\n",
    "def polarity(row):\n",
    "   \n",
    "   doc = nlp(row['concatenated_text']) # collecting the data of the given row in a loop\n",
    "   \n",
    "   \n",
    "   \n",
    "   return doc._.blob.polarity # putting the text through Sentiment Analyses to get its polarity\n",
    "\n",
    "    \n",
    "\n",
    "polarity\n",
    "df_hispanic['polarity'] = df_hispanic.apply(polarity, axis=1) # new coloumn to show the polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hispanic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes text and turns it into a wordcloud. A word cloud is a cluster of words shown as an image that has more frequently used words larger and closer to the middle. This allows us to see common words, and given the context we can see if there is a common problem, or something everyone agrees on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_string = \" \".join(df_hispanic['concatenated_text'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100, mask=None, contour_width=3, contour_color='steelblue').generate(long_string)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wordcloud showed that the most common words were words like Arlington, community, school. Although the wordcloud shows what words were used the most, it does not show if they were used in a positive or negative way. This is why the previous code using Sentiment Analyses is important because it gives context on the positivity or negativity used in the responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code uses seaborn to show the data in a histogram. This graph shows the polarity of each answer as a bar graph. This allows visualization of how many comments were negative and how many were positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=df_hispanic, x='polarity')\n",
    "plt.title('Sentiment Analyses')\n",
    "plt.xlabel('polairty')\n",
    "plt.ylabel('amount of answers')\n",
    "plt.grid(True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first block of code prepares the spell checker for words it does not have like \"Arlington\", or \"VA\". These words are important to have given that the dataset is about Arlington Virginia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "spell.word_frequency.load_words([\n",
    "    'Arlington'\n",
    "    , 'Glebe'\n",
    "    , 'Ballston'\n",
    "    , 'Rosslyn'\n",
    "    , 'Pershing'\n",
    "    , 'Rockville'\n",
    "    , 'MD'\n",
    "    , 'VA'\n",
    "    , 'Maryland'\n",
    "    , 'Virginia'\n",
    "    , 'Bluemont'\n",
    "    , 'Wilson'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs pyspellchecker to correct the grammar of the text. The given text is split into words with tokenization and then run through a spacy word databse to find if the word needs to be corrected or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(text):\n",
    "    doc = nlp(text)  # Process the text with spaCy\n",
    "    corrected_words = []\n",
    "    \n",
    "    # Find misspelled words\n",
    "    misspelled = spell.unknown([token.text for token in doc if not token.is_punct and not token.is_stop])\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_stop and len(token.text.strip()) > 0:  # Exclude punctuation and stop words\n",
    "            word = token.text.strip()\n",
    "            if word.lower() in misspelled:\n",
    "                correction = spell.correction(word)\n",
    "                if (correction is not None) and (correction.lower() != word.lower()):\n",
    "                    corrected_words.append(correction)\n",
    "                    #Uncomment this line to review the list of words that are correcting\n",
    "                    #print(f\"Correcting {word} => {correction}\")\n",
    "                else:\n",
    "                   corrected_words.append(word.lower())\n",
    "            else:\n",
    "                corrected_words.append(word)  # Preserve correct words\n",
    "\n",
    "    if len(corrected_words)>0:\n",
    "        return \" \".join(corrected_words)\n",
    "    else: \n",
    "        return \"\"\n",
    "\n",
    "df_hispanic['spell_checked_text'] = df_hispanic['concatenated_text'].apply(spell_check)\n",
    "df_hispanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loops through each response and compares how similar they are to a given query. Using a problem belived to show up in the data, the code compares the word vector from the prompt to the data. The data with the closest vector is the output given how similar it is to the query. Given the query might not have one or more responses similar to it, the data with words whom's vector is similar to it gets selected as an output. One example of this could be the word \"bus\" being a result with a query that uses the word \"school\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"missing middle\"\n",
    "def similarityToQuery(text):\n",
    "    return nlp(text).similarity(nlp(query))\n",
    "df_hispanic['similarity_to_query'] = df_hispanic['spell_checked_text'].apply(similarityToQuery)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df_hispanic.sort_values('similarity_to_query', ascending=False).iloc[0][\"concatenated_text\"])\n",
    "print(df_hispanic.sort_values('similarity_to_query', ascending=False).iloc[1][\"concatenated_text\"])\n",
    "print(df_hispanic.sort_values('similarity_to_query', ascending=False).iloc[2][\"concatenated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project started off with importing and clceaning an excel file. Once that was done, I used charts and wordclouds to illustrate what was most common amongst the answers, and if they were postivie or negative. I also inlcuded a tool that works similar to a search engine by pulling the most similar responses to any prompt. I specifically focused on Sentiment Analyses which focuses on polarity and objectivity. I belive that it was very neccesary for this project becuase without it we could only know what the responses were about, but not what the people think about it, and that is very important to me. This is my first project using surveys and data science, I am happy with the results and look forward to more work along the lines of data and how to deal with it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
